{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor & matrix factorization 2020 Final project\n",
    "\n",
    "Here the code for decomposition algorithm is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorly as tl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights extraction from ResNet-18 (layer 4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.cuda()\n",
    "summary(resnet18, input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in resnet18.named_parameters():\n",
    "    print(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = resnet18.layer4[1].conv1.weight.data.detach().cpu().clone().numpy()\n",
    "Y = Y.reshape(512,512,9).transpose([2,0,1])\n",
    "Y = np.transpose(Y, [1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 50\n",
    "R1, R2 = 30,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempts to implement [this article by AH Phan et al.](https://www.researchgate.net/publication/343626375_Stable_Low-rank_Tensor_Decomposition_for_Compression_of_Convolutional_Neural_Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.diag(np.ones(90))[:,:R1]\n",
    "V = np.diag(np.ones(90))[:,:R2]\n",
    "A = np.random.randn(R1,R)\n",
    "B = np.random.randn(R2,R)\n",
    "C = np.random.randn(9,R)\n",
    "\n",
    "#Y_restored = np.transpose(tl.kruskal_to_tensor((np.ones(R), (U@A, V@B, C))), [0,1,2])\n",
    "\n",
    "#print(\"Random tensors. Norm is\", np.linalg.norm(Y-Y_restored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y1 = tl.unfold(core,0)\n",
    "for i in range(20):\n",
    "    #print(\"Relative Norm is\", np.linalg.norm((U@A @ (tl.tenalg.khatri_rao([C, V@B])).T) - Y1)/\\\n",
    "         # np.linalg.norm(Y1))\n",
    "    A = U.T @ tl.unfold(core,0) @ (tl.tenalg.khatri_rao([C, V@B])) @ np.linalg.inv(C.T@C * (B.T@B))\n",
    "    Q = tl.unfold(core,0) @ (tl.tenalg.khatri_rao([C, V@B]))@ np.linalg.inv((C.T@C * (B.T@B))) @\\\n",
    "        tl.tenalg.khatri_rao([C, V@B]).T @ tl.unfold(core,0).T\n",
    "\n",
    "    eigvals, eigvecs = np.linalg.eigh(Q)\n",
    "    indices = np.argsort(np.abs(eigvals))[-R1:]\n",
    "    \n",
    "    U = eigvecs[:,indices]\n",
    "    \n",
    "    B = V.T @ tl.unfold(core,1) @ (tl.tenalg.khatri_rao([C, U@A])) @ np.linalg.inv(C.T@C * (A.T@A))\n",
    "    \"\"\"\n",
    "    R_ = tl.unfold(core,1) @ tl.tenalg.khatri_rao([C, U@A]) @ np.linalg.inv(C.T@C * (A.T@A)) @ \\\n",
    "        tl.tenalg.khatri_rao([C, U@A]).T@tl.unfold(core,1).T\n",
    "    eigvals, eigvecs = np.linalg.eig(R_)\n",
    "    indices = np.argsort(np.abs(eigvals))[-R2:]\n",
    "    V = eigvecs[:,indices]\n",
    "    \n",
    "    C = tl.unfold(core,2) @ (tl.tenalg.khatri_rao([U@A, V@B])) @ np.linalg.inv(B.T@B * (A.T@A))\n",
    "    #C[C < 0] = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Relative Norm is\", np.linalg.norm((U@A @ (tl.tenalg.khatri_rao([C, V@B])).T) - Y1)/\\\n",
    "          np.linalg.norm(Y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this code did not converge in terms of norm of difference of restored and original tensor. Therefore I switched to less sophisticated version of TKD_CPD compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TKD-CPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import partial_tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = resnet18.layer4[1].conv1.weight.data.detach().cpu().clone().numpy()\n",
    "Y = Y.reshape(512, 512, 9)\n",
    "Y = Y.transpose((2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```conduct_compression(Y, r1, r2, R)``` conducts tensor decompositions and saves results to corresponding binary ```*.npy``` files. There is no need to launch it as all decompositions were saved to [google drive](https://drive.google.com/drive/folders/11N0KO7ooM6pZomPOz9BiY6pq1deBJbwO).\n",
    "\n",
    "Next cells are written to show which ranks combinations were computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_compression(Y, r1,r2,R):\n",
    "    core, factors = partial_tucker(Y, \n",
    "                                   modes=[1,2],\n",
    "                                   ranks=[r1,r2],\n",
    "                                   n_iter_max=200, \n",
    "                                   tol=1e-6)\n",
    "    np.save(f\"core_{r1}_{r2}_{R}\", core)\n",
    "    print(\"core:\", core.shape)\n",
    "    for i in range(len(factors)):\n",
    "        np.save(f\"factor_{i}_{r1}_{r2}_{R}\" , factors[i])\n",
    "        print(\"fi\",factors[i].shape)\n",
    "    core_weights, core_factors = tl.decomposition.parafac(core,\n",
    "                             rank=R, \n",
    "                             n_iter_max=5000,\n",
    "                             tol=1e-7)\n",
    "    #print(type(core_factors), len(core_factors))\n",
    "    np.save(f\"core_weights_{r1}_{r2}_{R}\", core_weights)\n",
    "    #print(\"core weights:\", core_weights.shape)\n",
    "    for i in range(len(core_factors)):\n",
    "        print(\"core fac\", core_factors[i].shape)\n",
    "        np.save(f\"core_factor_{i}_{r1}_{r2}_{R}\", core_factors[i])\n",
    "        #print(\"core fact:\", core_factors[i].shape)\n",
    "    restored_core = tl.kruskal_to_tensor((core_weights, core_factors))\n",
    "    #print(\"rest core:\", restored_core.shape)\n",
    "    np.linalg.norm(core-restored_core)/np.linalg.norm(core)\n",
    "\n",
    "    restored_conv = np.zeros_like(Y)\n",
    "    restored_conv = np.zeros_like(Y)\n",
    "    for i in range(9):\n",
    "        restored_conv[i] = tl.tucker_to_tensor((restored_core[i], factors))\n",
    "\n",
    "    return np.linalg.norm(restored_conv-Y)/np.linalg.norm(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = {}\n",
    "for R in [100]:\n",
    "    norm = conduct_compression(Y, 90,80,R)\n",
    "    norms[R] = norm\n",
    "norms_aggr[\"90 90\"] = norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = {}\n",
    "for R in [100, 120, 150, 180, 210, 240, 250, 280, 300, 330, 380, 400]:\n",
    "    norm = conduct_compression(Y, 90,90,R)\n",
    "    norms[R] = norm\n",
    "norms_aggr[\"90 90\"] = norqms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r1 in (60,):\n",
    "    print()\n",
    "    print(\"r1 is \", r1, end=\"\")\n",
    "    for r2 in (30,60,90):\n",
    "        print(r2, end = \" \")\n",
    "        for R in [100, 120, 150, 180, 240, 250]:\n",
    "            norm = conduct_compression(Y, r1, r2,R)\n",
    "            norms[R] = norm\n",
    "        norms_aggr[str(r1) +\" \"+str(r2)] = norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse decompositions, you should download ```npy``` files and place them to the folder ```decompositions``` in which is located in the same dir with current ```ipynb```.\n",
    "\n",
    "The code below first checks if all decompositions from the ranks combination are present in the folder ```decompositions```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r1 in (30,60,90):\n",
    "    for r2 in (30,60,90):\n",
    "        for R in [100, 120, 150, 180, 240, 250]:\n",
    "            assert os.path.exists(f\"./decompositions/core_{r1}_{r2}_{R}.npy\")\n",
    "            print(r1,r2,R)\n",
    "            for i in range(2):\n",
    "                print(i)\n",
    "                assert os.path.exists(f\"./decompositions/factor_{i}_{r1}_{r2}_{R}.npy\")\n",
    "            assert os.path.exists(f\"./decompositions/core_weights_{r1}_{r2}_{R}.npy\")\n",
    "            for i in range(3):\n",
    "                assert os.path.exists(f\"./decompositions/core_factor_{i}_{r1}_{r2}_{R}.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above is successfully launched, we can then proceed to reconstruction of tensors. Moreover, we can launch then the next ```ipynb``` which is called ```resnet.ipynb```. It will parse decompositions and replace conv layers in the original NN with their compressed convs with corresponding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = np.load(f\"./decompositions/core_{r1}_{r2}_{R}.npy\")\n",
    "factors = []\n",
    "\n",
    "for i in range(3):\n",
    "        factors.append(np.load(f\"./decompositions/factor_{i}_{r1}_{r2}_{R}.npy\"))\n",
    "\n",
    "core_weights = np.load(f\"./decompositions/core_weights_{r1}_{r2}_{R}.npy\")\n",
    "core_factors = []\n",
    "\n",
    "for i in range(2):\n",
    "    core_factors.append(np.load(f\"./decompositions/core_factor_{i}_{r1}_{r2}_{R}.npy\"))\n",
    "core_factors = tuple(core_factors)\n",
    "restored_core = tl.kruskal_to_tensor((core_weights, core_factors))\n",
    "print(restored_core.shape)\n",
    "    #np.linalg.norm(core-restored_core)/np.linalg.norm(core)\n",
    "restored_conv = np.zeros_like(Y)\n",
    "print(restored_conv.shape)\n",
    "\n",
    "for i in range(9):\n",
    "    print(i)\n",
    "    restored_conv[i] = tl.tucker_to_tensor((restored_core[i], factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the simple function which counts the compression rate, i.e. ratio of n_parameters in the original layers to the number of them in compressed kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompressionRate(r1,r2,R):\n",
    "    s = 9*r1*r2 + 512*(r1+r2) + 9*R + R*(r1+r2)\n",
    "    init = 512*512*9.\n",
    "    return init/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompressionRate(90, 90, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
